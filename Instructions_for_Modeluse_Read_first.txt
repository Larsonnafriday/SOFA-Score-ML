
All models available on GitHub: https://github.com/Larsonnafriday/SOFA-Score-ML.git

This directory contains the ML models of the paper "Assessing SOFA score trajectories in sepsis using machine learning:
A pragmatic approach to improve the accuracy of mortality prediction."


## All models are included in the supplements of the paper as '.txt' file (conversion necessary), 
## can be accessed via GitHub (see above) or are available from the corresponding author.

Instructions follow on how the reader can obtain information about each of these models and how they can be applied for testing purposes.

All our ML models have been generated with The R Project version (The R Foundation for Statistical Computing, Vienna, Austria) and can be run using this program.


1. load the models into your R environment with the command:
## model_name <- readRDS("PATH/TO/YOUR/Model/Model_3days_svmPoly_new") / "PATH TO YOUR MODEL" is the location of the individual models.
## only when using the models included in the supplements of the paper type: readRDS("PATH/TO/YOUR/Model/Model_3days_svmPoly_new.txt")

2. for an insight into the generation of the final models, including the hyperparameters follow the command:
## print(model_name) #see Example1

3. for an insight into the resamples of the 5x repeated 10-fold cross validation follow the command:
## model_name$resample #see Example2

4. an explanation including an example dataset can be found in the file: Example_Script (included in the Supplements or available on GitHub)

### Example 1
## Insight into Model
## Artificial Neural Net for assessment of the first 5 Days / SOFA Scores
## ann5 <- readRDS("PATH/TO/YOUR/Model/Model_5days_artificialNeuralNet_new)
## print(ann5)

## You will get following results


Neural Network 1790 samples   5 predictor   2 classes: 'alive', 'dead' No pre-processingResampling: Cross-Validated (10 fold, repeated 5 times) Summary of sample sizes: 1611, 1611, 1611, 1612, 1611, 1610, ... Resampling results across tuning parameters:  size  decay  logLoss    AUC        prAUC      Accuracy   Kappa      F1         Sensitivity  Specificity  1     0e+00  0.5115402  0.6681105  0.4994636  0.7600918  0.1914364  0.8547503  0.9509228    0.2117391    1     1e-04  0.5043259  0.6742604  0.5251703  0.7632154  0.2095791  0.8561349  0.9488209    0.2298150    1     1e-01  0.4781578  0.7710097  0.7181481  0.7724862  0.3014863  0.8579154  0.9258874    0.3314709    3     0e+00  0.5032551  0.7577997  0.6897006  0.7708214  0.2910956  0.8570900  0.9261916    0.3240333    3     1e-04  0.5006126  0.7583969  0.7006468  0.7711479  0.2838870  0.8580388  0.9320654    0.3086031    3     1e-01  0.4831849  0.7649941  0.7119018  0.7709369  0.2916330  0.8573765  0.9278526    0.3198520    5     0e+00  0.5343577  0.7637483  0.7077074  0.7727159  0.3017954  0.8580487  0.9258817    0.3323682    5     1e-04  0.5084605  0.7660509  0.7082858  0.7710374  0.2956332  0.8571137  0.9255935    0.3267068    5     1e-01  0.4879006  0.7593340  0.7044855  0.7688021  0.2875702  0.8558451  0.9248371    0.3202220    Pos_Pred_Value  Neg_Pred_Value  Precision  Recall     Detection_Rate  Balanced_Accuracy  0.7781071       0.6024263       0.7781071  0.9509228  0.7054794       0.5813309          0.7817874       0.6108862       0.7817874  0.9488209  0.7039170       0.5893180          0.7994434       0.6102754       0.7994434  0.9258874  0.6869110       0.6286792          0.7981959       0.6054478       0.7981959  0.9261916  0.6871469       0.6251125          0.7954195       0.6135199       0.7954195  0.9320654  0.6914921       0.6203343          0.7970599       0.6059827       0.7970599  0.9278526  0.6883741       0.6238523          0.7997847       0.6099381       0.7997847  0.9258817  0.6869116       0.6291250          0.7983093       0.6048119       0.7983093  0.9255935  0.6866938       0.6261501          0.7966212       0.5962779       0.7966212  0.9248371  0.6861307       0.6225296        AUC was used to select the optimal model using the largest value.The final values used for the model were size = 1 and decay = 0.1.


### Example 2
## Insight into 5x repeated 10fold cross validation
## Support Vector Machine With Polynomial Kernel for assessment of the first 7 Days / SOFA Scores
## svm7 <- readRDS("PATH/TO/YOUR/Model/Model_7days_svmPoly_new)
## svm7$resample

## You will get following results of cross validation (n=50)

     logLoss       AUC     prAUC  Accuracy      Kappa        F1 Sensitivity Specificity Pos_Pred_Value1  0.4576630 0.8008893 0.7349539 0.7865169 0.35111282 0.8661972   0.9318182  0.36956522      0.80921052  0.5041267 0.7286834 0.6963172 0.7666667 0.23805684 0.8581081   0.9548872  0.23404255      0.77914113  0.4618155 0.8093905 0.7303735 0.7611111 0.23968566 0.8532423   0.9398496  0.25531915      0.78125004  0.5343711 0.7034750 0.6375243 0.7359551 0.17706079 0.8362369   0.9090909  0.23913043      0.77419355  0.5118543 0.8170971 0.7758472 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.74301686  0.4978892 0.7595906 0.7057621 0.7932961 0.43116035 0.8644689   0.8939394  0.51063830      0.83687947  0.5666506 0.8099052 0.7825297 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.74301688  0.5036499 0.7335626 0.6550196 0.7611111 0.21309475 0.8552189   0.9548872  0.21276596      0.77439029  0.4536851 0.8170154 0.7467348 0.7877095 0.32972014 0.8689655   0.9473684  0.32608696      0.802547810 0.4997508 0.7261360 0.6843829 0.7765363 0.24248836 0.8657718   0.9699248  0.21739130      0.781818211 0.4762179 0.7851614 0.7186495 0.7640449 0.21831870 0.8571429   0.9545455  0.21739130      0.777777812 0.4561652 0.8041026 0.7285959 0.7765363 0.29444225 0.8620690   0.9398496  0.30434783      0.796178313 0.5290140 0.6855181 0.6206414 0.7653631 0.20461278 0.8590604   0.9624060  0.19565217      0.775757614 0.4941101 0.7373325 0.6854026 0.8044693 0.38788471 0.8788927   0.9548872  0.36956522      0.814102615 0.5639001 0.8772475 0.8348698 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.743016816 0.5022725 0.7999346 0.7631609 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.743016817 0.5043343 0.7323472 0.6704401 0.7486034 0.21299463 0.8442907   0.9172932  0.26086957      0.782051318 0.4748472 0.8021412 0.7589053 0.7709497 0.17007803 0.8655738   0.9924812  0.13043478      0.767441919 0.5049586 0.7274836 0.6848839 0.7777778 0.28670497 0.8639456   0.9548872  0.27659574      0.788819920 0.5513253 0.8237882 0.7745167 0.7388889 0.00000000 0.8498403   1.0000000  0.00000000      0.738888921 0.4560354 0.7967721 0.7464526 0.7865169 0.31731934 0.8689655   0.9545455  0.30434783      0.797468422 0.4815198 0.7692056 0.7181587 0.7709497 0.24441470 0.8610169   0.9548872  0.23913043      0.783950623 0.5138083 0.7242563 0.6625083 0.7653631 0.24624022 0.8561644   0.9398496  0.26086957      0.786163524 0.4705636 0.7795848 0.7259901 0.7821229 0.31792868 0.8650519   0.9398496  0.32608696      0.801282125 0.4719912 0.7753351 0.7055058 0.7765363 0.31783537 0.8601399   0.9248120  0.34782609      0.803921626 0.4841969 0.7745389 0.7240874 0.7808989 0.22539612 0.8704319   0.9924242  0.17391304      0.775147927 0.5447019 0.8127656 0.7611150 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.743016828 0.4838061 0.7613599 0.7114177 0.7486034 0.18529382 0.8464164   0.9323308  0.21739130      0.775000029 0.4742958 0.7824348 0.7380022 0.7444444 0.27811683 0.8345324   0.8721805  0.38297872      0.800000030 0.4817417 0.7715744 0.6980547 0.7303371 0.18035303 0.8309859   0.8939394  0.26086957      0.776315831 0.5491923 0.6718925 0.6390990 0.7444444 0.16549083 0.8445946   0.9398496  0.19148936      0.766871232 0.4856224 0.8322164 0.7535454 0.7541899 0.08226521 0.8571429   0.9924812  0.06521739      0.754285733 0.5232780 0.7131917 0.6601157 0.7471910 0.19851911 0.8442907   0.9242424  0.23913043      0.777070134 0.5505063 0.8355672 0.7717689 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.743016835 0.5181093 0.7206047 0.6628177 0.7388889 0.22156791 0.8350877   0.8947368  0.29787234      0.782894736 0.5046791 0.7305492 0.6836124 0.7486034 0.26310493 0.8398577   0.8872180  0.34782609      0.797297337 0.4841024 0.7459137 0.7122552 0.7765363 0.25618118 0.8648649   0.9624060  0.23913043      0.785276138 0.4635799 0.8047565 0.7478842 0.7765363 0.26938776 0.8639456   0.9548872  0.26086957      0.788819939 0.4933374 0.7723112 0.7319083 0.7821229 0.24045262 0.8704319   0.9849624  0.19565217      0.779761940 0.4664882 0.7825270 0.7500260 0.7988827 0.33056306 0.8783784   0.9774436  0.28260870      0.797546041 0.5207380 0.8183491 0.7589432 0.7388889 0.00000000 0.8498403   1.0000000  0.00000000      0.738888942 0.4926798 0.8160347 0.7552114 0.7430168 0.00000000 0.8525641   1.0000000  0.00000000      0.743016843 0.4976459 0.7491010 0.6910659 0.7597765 0.27270150 0.8491228   0.9097744  0.32608696      0.796052644 0.4813581 0.7656914 0.7157242 0.7597765 0.28446593 0.8480565   0.9022556  0.34782609      0.800000045 0.5009387 0.8085159 0.7591181 0.7541899 0.06327307 0.8580645   1.0000000  0.04347826      0.751412446 0.4826705 0.7771739 0.7125807 0.7640449 0.33746898 0.8467153   0.8787879  0.43478261      0.816901447 0.4744147 0.7719843 0.7035657 0.7988827 0.38605183 0.8741259   0.9398496  0.39130435      0.816993548 0.4850271 0.8203228 0.7639878 0.7752809 0.18198529 0.8684211   1.0000000  0.13043478      0.767441949 0.5153436 0.6972867 0.6663738 0.7597765 0.23496670 0.8522337   0.9323308  0.26086957      0.784810150 0.4779507 0.7802386 0.7109266 0.7653631 0.17420914 0.8609272   0.9774436  0.15217391      0.7692308   Neg_Pred_Value Precision    Recall Detection_Rate Balanced_Accuracy    Resample1       0.6538462 0.8092105 0.9318182      0.6910112         0.6506917 Fold06.Rep52       0.6470588 0.7791411 0.9548872      0.7055556         0.5944649 Fold05.Rep53       0.6000000 0.7812500 0.9398496      0.6944444         0.5975844 Fold03.Rep44       0.4782609 0.7741935 0.9090909      0.6741573         0.5741107 Fold10.Rep25             NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold07.Rep56       0.6315789 0.8368794 0.8939394      0.6592179         0.7022888 Fold04.Rep47             NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold02.Rep48       0.6250000 0.7743902 0.9548872      0.7055556         0.5838266 Fold09.Rep29       0.6818182 0.8025478 0.9473684      0.7039106         0.6367277 Fold07.Rep110      0.7142857 0.7818182 0.9699248      0.7206704         0.5936581 Fold06.Rep111      0.6250000 0.7777778 0.9545455      0.7078652         0.5859684 Fold01.Rep312      0.6363636 0.7961783 0.9398496      0.6983240         0.6220987 Fold04.Rep513      0.6428571 0.7757576 0.9624060      0.7150838         0.5790291 Fold08.Rep214      0.7391304 0.8141026 0.9548872      0.7094972         0.6622262 Fold07.Rep215            NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold04.Rep216            NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold08.Rep517      0.5217391 0.7820513 0.9172932      0.6815642         0.5890814 Fold01.Rep418      0.8571429 0.7674419 0.9924812      0.7374302         0.5614580 Fold10.Rep319      0.6842105 0.7888199 0.9548872      0.7055556         0.6157415 Fold08.Rep320            NaN 0.7388889 1.0000000      0.7388889         0.5000000 Fold08.Rep121      0.7000000 0.7974684 0.9545455      0.7078652         0.6294466 Fold05.Rep422      0.6470588 0.7839506 0.9548872      0.7094972         0.5970088 Fold02.Rep323      0.6000000 0.7861635 0.9398496      0.6983240         0.6003596 Fold09.Rep524      0.6521739 0.8012821 0.9398496      0.6983240         0.6329683 Fold06.Rep225      0.6153846 0.8039216 0.9248120      0.6871508         0.6363191 Fold04.Rep126      0.8888889 0.7751479 0.9924242      0.7359551         0.5831686 Fold03.Rep527            NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold09.Rep328      0.5263158 0.7750000 0.9323308      0.6927374         0.5748611 Fold05.Rep229      0.5142857 0.8000000 0.8721805      0.6444444         0.6275796 Fold02.Rep230      0.4615385 0.7763158 0.8939394      0.6629213         0.5774045 Fold03.Rep131      0.5294118 0.7668712 0.9398496      0.6944444         0.5656695 Fold09.Rep132      0.7500000 0.7542857 0.9924812      0.7374302         0.5288493 Fold06.Rep433      0.5238095 0.7770701 0.9242424      0.6853933         0.5816864 Fold06.Rep334            NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold01.Rep235      0.5000000 0.7828947 0.8947368      0.6611111         0.5963046 Fold02.Rep536      0.5161290 0.7972973 0.8872180      0.6592179         0.6175221 Fold10.Rep437      0.6875000 0.7852761 0.9624060      0.7150838         0.6007682 Fold03.Rep338      0.6666667 0.7888199 0.9548872      0.7094972         0.6078784 Fold10.Rep539      0.8181818 0.7797619 0.9849624      0.7318436         0.5903073 Fold02.Rep140      0.8125000 0.7975460 0.9774436      0.7262570         0.6300262 Fold05.Rep141            NaN 0.7388889 1.0000000      0.7388889         0.5000000 Fold07.Rep342            NaN 0.7430168 1.0000000      0.7430168         0.5000000 Fold04.Rep343      0.5555556 0.7960526 0.9097744      0.6759777         0.6179307 Fold05.Rep344      0.5517241 0.8000000 0.9022556      0.6703911         0.6250409 Fold01.Rep545      1.0000000 0.7514124 1.0000000      0.7430168         0.5217391 Fold01.Rep146      0.5555556 0.8169014 0.8787879      0.6516854         0.6567852 Fold10.Rep147      0.6923077 0.8169935 0.9398496      0.6983240         0.6655770 Fold07.Rep448      1.0000000 0.7674419 1.0000000      0.7415730         0.5652174 Fold03.Rep249      0.5714286 0.7848101 0.9323308      0.6927374         0.5966002 Fold09.Rep450      0.7000000 0.7692308 0.9774436      0.7262570         0.5648088 Fold08.Rep4